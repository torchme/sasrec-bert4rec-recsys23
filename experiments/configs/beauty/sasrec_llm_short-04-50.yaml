seed: 42

model:
  model_name: BERT4RecLLM # SASRecLLM #
  item_num: 0  # Будет обновлено в коде
  user_num: 0
  maxlen: 50
  hidden_units: 128
  num_blocks: 2
  num_heads: 2
  dropout_rate: 0.2
  initializer_range: 0.02
  add_head: True
  reconstruction_layer: -1  # Используем последний слой для реконструкции
  weighting_scheme: mean
  use_down_scale: true
  use_upscale: false

training:
  batch_size: 64
  epochs: 1
  learning_rate: 0.0001
  eval_every: 1
  model_dir: models/
  alpha: 0.4  # Параметр для комбинированной функции потерь
  fine_tune_epoch: 1  # Эпоха, после которой начинаем тонкую настройку
  reconstruct_loss: MSE

data:
  train_sequences: /home/danya/sasrec-bert4rec-recsys23/data/raw/amazon_beauty/train_sequences.pkl
  valid_sequences: /home/danya/sasrec-bert4rec-recsys23/data/raw/amazon_beauty/valid_sequences.pkl
  test_sequences: /home/danya/sasrec-bert4rec-recsys23/data/raw/amazon_beauty/test_sequences.pkl
  mappings: /home/danya/sasrec-bert4rec-recsys23/data/raw/amazon_beauty/mappings.pkl
  counts: /home/danya/sasrec-bert4rec-recsys23/data/raw/amazon_beauty/counts.pkl
  user_profile_embeddings_path: /home/danya/sasrec-bert4rec-recsys23/data/raw/amazon_beauty/short_embedding_amazon_beauty_UMAP-128.json

experiment_name: "SASRecLLM Beauty Experiment - UMAP-128"
