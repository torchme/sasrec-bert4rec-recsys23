{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SASRec with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (697378, 4)\n",
      "Valid size: (99582, 4)\n",
      "Test size: (203165, 4)\n",
      "Количество пользователей в обучающем наборе: 6040\n",
      "Количество пользователей в валидационном наборе: 5954\n",
      "Количество пользователей в тестовом наборе: 6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redpo\\Desktop\\ITMO\\ITMO RESEARCH\\sasrec-bert4rec-recsys23\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 7.9700, Valid Loss: 7.3887\n",
      "Epoch 2/10, Train Loss: 6.9164, Valid Loss: 6.6426\n",
      "Epoch 3/10, Train Loss: 6.1473, Valid Loss: 5.8828\n",
      "Epoch 4/10, Train Loss: 5.3706, Valid Loss: 5.1023\n",
      "Epoch 5/10, Train Loss: 4.5954, Valid Loss: 4.4545\n",
      "Epoch 6/10, Train Loss: 3.9272, Valid Loss: 3.8361\n",
      "Epoch 7/10, Train Loss: 3.3532, Valid Loss: 3.3553\n",
      "Epoch 8/10, Train Loss: 2.8526, Valid Loss: 2.9449\n",
      "Epoch 9/10, Train Loss: 2.4329, Valid Loss: 2.6268\n",
      "Epoch 10/10, Train Loss: 2.0553, Valid Loss: 2.4178\n",
      "Precision@10: 0.0120\n",
      "Recall@10: 0.1199\n",
      "NDCG@10: 0.0571\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "# Параметры\n",
    "max_len = 50        # Максимальная длина последовательности\n",
    "batch_size = 128    # Размер батча\n",
    "num_negatives = 300  # Количество негативных сэмплов\n",
    "\n",
    "# Файлы данных\n",
    "train_file = '../data/source/1_ml-1m_original.part1.inter'\n",
    "valid_file = '../data/source/1_ml-1m_original.part2.inter'\n",
    "test_file = '../data/source/1_ml-1m_original.part3.inter'\n",
    "\n",
    "# Загрузка данных\n",
    "train_data = pd.read_csv(train_file, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'], skiprows=1)\n",
    "valid_data = pd.read_csv(valid_file, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'], skiprows=1)\n",
    "test_data = pd.read_csv(test_file, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'], skiprows=1)\n",
    "\n",
    "print(f'Train size: {train_data.shape}')\n",
    "print(f'Valid size: {valid_data.shape}')\n",
    "print(f'Test size: {test_data.shape}')\n",
    "\n",
    "# Для отображения первых строк DataFrame (только если вы работаете в Jupyter Notebook)\n",
    "# display(train_data.head())\n",
    "\n",
    "# Подготовка последовательностей для обучения, валидации и теста\n",
    "def prepare_sequences(data):\n",
    "    user_group = data.groupby('user_id')['item_id'].apply(list)\n",
    "    sequences = []\n",
    "    user_ids = []\n",
    "    for user_id, seq in user_group.items():\n",
    "        if len(seq) >= 2:  # Только пользователи с достаточной историей\n",
    "            sequences.append(seq)\n",
    "            user_ids.append(user_id)\n",
    "    return sequences, user_ids\n",
    "\n",
    "train_sequences, train_user_ids = prepare_sequences(train_data)\n",
    "valid_sequences, valid_user_ids = prepare_sequences(valid_data)\n",
    "test_sequences, test_user_ids = prepare_sequences(test_data)\n",
    "\n",
    "print(f'Количество пользователей в обучающем наборе: {len(train_sequences)}')\n",
    "print(f'Количество пользователей в валидационном наборе: {len(valid_sequences)}')\n",
    "print(f'Количество пользователей в тестовом наборе: {len(test_sequences)}')\n",
    "\n",
    "# Создание набора всех элементов\n",
    "num_items = max(train_data['item_id'].max(), valid_data['item_id'].max(), test_data['item_id'].max())\n",
    "all_items = set(range(1, num_items + 1))\n",
    "\n",
    "# Создание словаря взаимодействий пользователей из всех наборов данных\n",
    "user_interactions = defaultdict(set)\n",
    "\n",
    "# Добавляем взаимодействия из обучающего набора\n",
    "for user_id, group in train_data.groupby('user_id'):\n",
    "    user_interactions[user_id].update(group['item_id'].tolist())\n",
    "\n",
    "# Добавляем взаимодействия из валидационного набора\n",
    "for user_id, group in valid_data.groupby('user_id'):\n",
    "    user_interactions[user_id].update(group['item_id'].tolist())\n",
    "\n",
    "# Добавляем взаимодействия из тестового набора\n",
    "for user_id, group in test_data.groupby('user_id'):\n",
    "    user_interactions[user_id].update(group['item_id'].tolist())\n",
    "\n",
    "# Функция для выборки негативных сэмплов\n",
    "def sample_negatives(user_id, num_negatives, all_items, user_interactions):\n",
    "    user_items = user_interactions.get(user_id, set())\n",
    "    negatives = []\n",
    "    while len(negatives) < num_negatives:\n",
    "        neg_item = np.random.randint(1, num_items + 1)\n",
    "        if neg_item not in user_items:\n",
    "            negatives.append(neg_item)\n",
    "    return negatives\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len=50):\n",
    "        self.sequences = sequences\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "\n",
    "        # Паддинг последовательности\n",
    "        if len(seq) < self.max_len:\n",
    "            padded_seq = [0] * (self.max_len - len(seq)) + seq\n",
    "            seq_len = len(seq)\n",
    "        else:\n",
    "            padded_seq = seq[-self.max_len:]\n",
    "            seq_len = self.max_len\n",
    "\n",
    "        return torch.tensor(padded_seq, dtype=torch.long), torch.tensor(seq_len, dtype=torch.long)\n",
    "\n",
    "# Создание датасетов и DataLoader\n",
    "train_dataset = MovieLensDataset(train_sequences, max_len)\n",
    "valid_dataset = MovieLensDataset(valid_sequences, max_len)\n",
    "test_dataset = MovieLensDataset(test_sequences, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Класс EvaluationDataset для валидации и тестирования с негативными сэмплами\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, sequences, user_ids, user_interactions, all_items, num_negatives=1000, max_len=50):\n",
    "        self.sequences = sequences\n",
    "        self.user_ids = user_ids\n",
    "        self.user_interactions = user_interactions\n",
    "        self.all_items = all_items\n",
    "        self.num_negatives = num_negatives\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        user_id = self.user_ids[idx]\n",
    "        pos_item = seq[-1]\n",
    "        seq_input = seq[:-1]\n",
    "\n",
    "        # Паддинг последовательности\n",
    "        if len(seq_input) < self.max_len:\n",
    "            padded_seq = [0] * (self.max_len - len(seq_input)) + seq_input\n",
    "            seq_len = len(seq_input)\n",
    "        else:\n",
    "            padded_seq = seq_input[-self.max_len:]\n",
    "            seq_len = self.max_len\n",
    "\n",
    "        # Выборка негативных сэмплов\n",
    "        negatives = sample_negatives(user_id, self.num_negatives, self.all_items, self.user_interactions)\n",
    "\n",
    "        # Список для оценки: 1 положительный + num_negatives негативных элементов\n",
    "        items = negatives + [pos_item]\n",
    "\n",
    "        return (torch.tensor(padded_seq, dtype=torch.long),\n",
    "                torch.tensor(seq_len, dtype=torch.long),\n",
    "                torch.tensor(items, dtype=torch.long),\n",
    "                torch.tensor(pos_item, dtype=torch.long))\n",
    "\n",
    "# Создание EvaluationDataset для валидации и тестирования\n",
    "valid_eval_dataset = EvaluationDataset(valid_sequences, valid_user_ids, user_interactions, all_items, num_negatives=num_negatives, max_len=max_len)\n",
    "test_eval_dataset = EvaluationDataset(test_sequences, test_user_ids, user_interactions, all_items, num_negatives=num_negatives, max_len=max_len)\n",
    "\n",
    "# Используем batch_size=1 для удобства обработки по пользователям\n",
    "valid_eval_loader = DataLoader(valid_eval_dataset, batch_size=1, shuffle=False)\n",
    "test_eval_loader = DataLoader(test_eval_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim=50, num_heads=2, num_layers=2, dropout=0.2, max_len=50):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)  # +1 для паддинга\n",
    "        self.position_embedding = nn.Embedding(max_len, embedding_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
    "                                                   nhead=num_heads,\n",
    "                                                   dropout=dropout,\n",
    "                                                   activation='relu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(embedding_dim, num_items + 1)\n",
    "\n",
    "    def forward(self, input_seq, seq_len):\n",
    "        # input_seq: (batch_size, max_len)\n",
    "        position_ids = torch.arange(0, input_seq.size(1), device=input_seq.device).unsqueeze(0).expand_as(input_seq)\n",
    "        item_emb = self.item_embedding(input_seq) + self.position_embedding(position_ids)\n",
    "\n",
    "        item_emb = self.layer_norm(item_emb)\n",
    "        item_emb = self.dropout(item_emb)\n",
    "\n",
    "        # Transformer ожидает ввод формы (seq_len, batch_size, embedding_dim)\n",
    "        item_emb = item_emb.transpose(0, 1)\n",
    "\n",
    "        # Создание маски для паддинга\n",
    "        mask = (input_seq == 0)  # (batch_size, max_len)\n",
    "\n",
    "        # Передача через Transformer\n",
    "        output = self.transformer(item_emb, src_key_padding_mask=mask)\n",
    "        output = output.transpose(0, 1)  # (batch_size, max_len, embedding_dim)\n",
    "\n",
    "        # Предсказание последнего элемента\n",
    "        output = output[:, -1, :]  # (batch_size, embedding_dim)\n",
    "        logits = self.fc(output)    # (batch_size, num_items + 1)\n",
    "        return logits\n",
    "\n",
    "# Параметры модели\n",
    "embedding_dim = 50\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "model = SASRec(num_items=num_items, embedding_dim=embedding_dim, num_heads=num_heads,\n",
    "              num_layers=num_layers, dropout=dropout, max_len=max_len)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Критерий и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Игнорируем паддинг\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Функции метрик\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    recommended = recommended[:k]\n",
    "    hits = len(set(recommended) & set(relevant))\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    recommended = recommended[:k]\n",
    "    hits = len(set(recommended) & set(relevant))\n",
    "    return hits / len(relevant) if relevant else 0\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    recommended = recommended[:k]\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in relevant:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Функция обучения\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        sequences, lengths = batch\n",
    "        sequences = sequences.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences, lengths)\n",
    "        targets = sequences[:, -1]  # Последний элемент последовательности\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Функция валидации\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            sequences, lengths = batch\n",
    "            sequences = sequences.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            outputs = model(sequences, lengths)\n",
    "            targets = sequences[:, -1]\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Функция оценки модели с негативными сэмплами\n",
    "def evaluate_model_with_negatives(model, loader, device, k=10):\n",
    "    model.eval()\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Каждый батч содержит одного пользователя\n",
    "            seq, seq_len, items, pos_item = batch\n",
    "            seq = seq.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            items = items.to(device)  # (batch_size=1, 1001)\n",
    "            pos_item = pos_item.to(device)  # (batch_size=1)\n",
    "\n",
    "            # Получение эмбеддингов и прогнозов модели для всех элементов\n",
    "            outputs = model(seq, seq_len)  # (batch_size=1, num_items +1)\n",
    "\n",
    "            # Извлечение оценок только для выбранных элементов (негативные + положительный)\n",
    "            # Предполагается, что items содержат индексы элементов\n",
    "            item_scores = outputs.gather(1, items)  # (batch_size=1, 1001)\n",
    "\n",
    "            # Получение top-K элементов среди негативных и положительного\n",
    "            _, topk_indices = torch.topk(item_scores, k, dim=1)  # (batch_size=1, k)\n",
    "            topk_items = items[0][topk_indices[0]].cpu().numpy()\n",
    "\n",
    "            # Положительный элемент находится в конце списка items\n",
    "            recommended = topk_items\n",
    "            relevant = [pos_item.item()]\n",
    "\n",
    "            precision_scores.append(precision_at_k(recommended, relevant, k))\n",
    "            recall_scores.append(recall_at_k(recommended, relevant, k))\n",
    "            ndcg_scores.append(ndcg_at_k(recommended, relevant, k))\n",
    "\n",
    "    # Вычисление средних значений метрик\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "\n",
    "    return mean_precision, mean_recall, mean_ndcg\n",
    "\n",
    "# Цикл обучения с валидацией\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss = validate(model, valid_loader, criterion, device)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "# Параметр K для top-K рекомендаций\n",
    "k = 10\n",
    "\n",
    "# Оценка модели на тестовом наборе с негативными сэмплами\n",
    "precision, recall, ndcg = evaluate_model_with_negatives(model, test_eval_loader, device, k=k)\n",
    "print(f'Precision@{k}: {precision:.4f}')\n",
    "print(f'Recall@{k}: {recall:.4f}')\n",
    "print(f'NDCG@{k}: {ndcg:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (697378, 4)\n",
      "Valid size: (99582, 4)\n",
      "Test size: (203165, 4)\n",
      "Количество пользователей в обучающем наборе: 6040\n",
      "Количество пользователей в валидационном наборе: 5954\n",
      "Количество пользователей в тестовом наборе: 6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\redpo\\Desktop\\ITMO\\ITMO RESEARCH\\sasrec-bert4rec-recsys23\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.7163, Valid Loss: 0.6016\n",
      "Epoch 2/10, Train Loss: 0.5178, Valid Loss: 0.4936\n",
      "Epoch 3/10, Train Loss: 0.3916, Valid Loss: 0.4591\n",
      "Epoch 4/10, Train Loss: 0.3052, Valid Loss: 0.4548\n",
      "Epoch 5/10, Train Loss: 0.2470, Valid Loss: 0.4477\n",
      "Epoch 6/10, Train Loss: 0.2078, Valid Loss: 0.4351\n",
      "Epoch 7/10, Train Loss: 0.1854, Valid Loss: 0.4420\n",
      "Epoch 8/10, Train Loss: 0.1649, Valid Loss: 0.4383\n",
      "Epoch 9/10, Train Loss: 0.1558, Valid Loss: 0.4406\n",
      "Epoch 10/10, Train Loss: 0.1472, Valid Loss: 0.4423\n",
      "Precision@10: 0.0184\n",
      "Recall@10: 0.1841\n",
      "NDCG@10: 0.0917\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "# Параметры\n",
    "max_len = 50           # Максимальная длина последовательности\n",
    "batch_size = 128       # Размер батча\n",
    "num_negatives = 300   # Количество негативных сэмплов\n",
    "external_embedding_dim = 1536  # Размерность внешних эмбеддингов\n",
    "projected_embedding_dim = 300   # Размерность после проекции внешних эмбеддингов\n",
    "embedding_dim = 50     # Размерность внутренних эмбеддингов модели SASRec\n",
    "lambda_align = 0.1     # Вес для alignment loss\n",
    "\n",
    "# Файлы данных\n",
    "train_file = '../data/source/1_ml-1m_original.part1.inter'\n",
    "valid_file = '../data/source/1_ml-1m_original.part2.inter'\n",
    "test_file = '../data/source/1_ml-1m_original.part3.inter'\n",
    "\n",
    "# Загрузка данных\n",
    "train_data = pd.read_csv(train_file, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'], skiprows=1)\n",
    "valid_data = pd.read_csv(valid_file, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'], skiprows=1)\n",
    "test_data = pd.read_csv(test_file, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'], skiprows=1)\n",
    "\n",
    "print(f'Train size: {train_data.shape}')\n",
    "print(f'Valid size: {valid_data.shape}')\n",
    "print(f'Test size: {test_data.shape}')\n",
    "\n",
    "# Подготовка последовательностей для обучения, валидации и теста\n",
    "def prepare_sequences(data):\n",
    "    user_group = data.groupby('user_id')['item_id'].apply(list)\n",
    "    sequences = []\n",
    "    user_ids = []\n",
    "    for user_id, seq in user_group.items():\n",
    "        if len(seq) >= 2:  # Только пользователи с достаточной историей\n",
    "            sequences.append(seq)\n",
    "            user_ids.append(user_id)\n",
    "    return sequences, user_ids\n",
    "\n",
    "train_sequences, train_user_ids = prepare_sequences(train_data)\n",
    "valid_sequences, valid_user_ids = prepare_sequences(valid_data)\n",
    "test_sequences, test_user_ids = prepare_sequences(test_data)\n",
    "\n",
    "print(f'Количество пользователей в обучающем наборе: {len(train_sequences)}')\n",
    "print(f'Количество пользователей в валидационном наборе: {len(valid_sequences)}')\n",
    "print(f'Количество пользователей в тестовом наборе: {len(test_sequences)}')\n",
    "\n",
    "# Определение количества пользователей и элементов\n",
    "num_users = max(train_data['user_id'].max(), valid_data['user_id'].max(), test_data['user_id'].max())\n",
    "num_items = max(train_data['item_id'].max(), valid_data['item_id'].max(), test_data['item_id'].max())\n",
    "\n",
    "# Создание набора всех элементов\n",
    "all_items = set(range(1, num_items + 1))\n",
    "\n",
    "# Создание словаря взаимодействий пользователей из всех наборов данных\n",
    "user_interactions = defaultdict(set)\n",
    "\n",
    "# Добавляем взаимодействия из обучающего набора\n",
    "for user_id, group in train_data.groupby('user_id'):\n",
    "    user_interactions[user_id].update(group['item_id'].tolist())\n",
    "\n",
    "# Добавляем взаимодействия из валидационного набора\n",
    "for user_id, group in valid_data.groupby('user_id'):\n",
    "    user_interactions[user_id].update(group['item_id'].tolist())\n",
    "\n",
    "# Добавляем взаимодействия из тестового набора\n",
    "for user_id, group in test_data.groupby('user_id'):\n",
    "    user_interactions[user_id].update(group['item_id'].tolist())\n",
    "\n",
    "# Функция для выборки негативных сэмплов\n",
    "def sample_negatives(user_id, num_negatives, all_items, user_interactions):\n",
    "    user_items = user_interactions.get(user_id, set())\n",
    "    negatives = []\n",
    "    while len(negatives) < num_negatives:\n",
    "        neg_item = np.random.randint(1, num_items + 1)\n",
    "        if neg_item not in user_items:\n",
    "            negatives.append(neg_item)\n",
    "    return negatives\n",
    "\n",
    "# Загрузка внешних эмбеддингов пользователей\n",
    "with open('../data/emb/embeddings.json', 'r') as f:\n",
    "    user_embeddings = json.load(f)\n",
    "\n",
    "# Преобразование эмбеддингов пользователей в словарь для быстрого доступа\n",
    "user2embedding = {int(user['id']): user['embedding'] for user in user_embeddings}\n",
    "\n",
    "# Проверка размерности внешних эмбеддингов\n",
    "if len(next(iter(user2embedding.values()))) != external_embedding_dim:\n",
    "    raise ValueError(f\"Размерность внешних эмбеддингов ({len(next(iter(user2embedding.values())))}), не совпадает с ожидаемой ({external_embedding_dim})\")\n",
    "\n",
    "# Класс Dataset для обучения и валидации, включающий user_id\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, sequences, user_ids, max_len=50):\n",
    "        self.sequences = sequences\n",
    "        self.user_ids = user_ids\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        user_id = self.user_ids[idx]\n",
    "\n",
    "        # Паддинг последовательности\n",
    "        if len(seq) < self.max_len:\n",
    "            padded_seq = [0] * (self.max_len - len(seq)) + seq\n",
    "            seq_len = len(seq)\n",
    "        else:\n",
    "            padded_seq = seq[-self.max_len:]\n",
    "            seq_len = self.max_len\n",
    "\n",
    "        return torch.tensor(user_id, dtype=torch.long), torch.tensor(padded_seq, dtype=torch.long), torch.tensor(seq_len, dtype=torch.long)\n",
    "\n",
    "# Класс EvaluationDataset для валидации и тестирования с негативными сэмплами\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, sequences, user_ids, user_interactions, all_items, user2embedding, num_negatives=1000, max_len=50):\n",
    "        self.sequences = sequences\n",
    "        self.user_ids = user_ids\n",
    "        self.user_interactions = user_interactions\n",
    "        self.all_items = all_items\n",
    "        self.num_negatives = num_negatives\n",
    "        self.max_len = max_len\n",
    "        self.user2embedding = user2embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        user_id = self.user_ids[idx]\n",
    "        pos_item = seq[-1]\n",
    "        seq_input = seq[:-1]\n",
    "\n",
    "        # Паддинг последовательности\n",
    "        if len(seq_input) < self.max_len:\n",
    "            padded_seq = [0] * (self.max_len - len(seq_input)) + seq_input\n",
    "            seq_len = len(seq_input)\n",
    "        else:\n",
    "            padded_seq = seq_input[-self.max_len:]\n",
    "            seq_len = self.max_len\n",
    "\n",
    "        # Выборка негативных сэмплов\n",
    "        negatives = sample_negatives(user_id, self.num_negatives, self.all_items, self.user_interactions)\n",
    "\n",
    "        # Список для оценки: num_negatives негативных элементов + 1 положительный элемент\n",
    "        items = negatives + [pos_item]\n",
    "\n",
    "        return (torch.tensor(user_id, dtype=torch.long),\n",
    "                torch.tensor(padded_seq, dtype=torch.long),\n",
    "                torch.tensor(seq_len, dtype=torch.long),\n",
    "                torch.tensor(items, dtype=torch.long),\n",
    "                torch.tensor(pos_item, dtype=torch.long))\n",
    "\n",
    "# Создание датасетов и DataLoader для обучения и валидации\n",
    "train_dataset = MovieLensDataset(train_sequences, train_user_ids, max_len)\n",
    "valid_dataset = MovieLensDataset(valid_sequences, valid_user_ids, max_len)\n",
    "test_dataset = MovieLensDataset(test_sequences, test_user_ids, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Создание EvaluationDataset для валидации и тестирования\n",
    "valid_eval_dataset = EvaluationDataset(valid_sequences, valid_user_ids, user_interactions, all_items, user2embedding, num_negatives=num_negatives, max_len=max_len)\n",
    "test_eval_dataset = EvaluationDataset(test_sequences, test_user_ids, user_interactions, all_items, user2embedding, num_negatives=num_negatives, max_len=max_len)\n",
    "\n",
    "# Используем batch_size=1 для удобства обработки по пользователям\n",
    "valid_eval_loader = DataLoader(valid_eval_dataset, batch_size=1, shuffle=False)\n",
    "test_eval_loader = DataLoader(test_eval_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Определение модели SASRec с интеграцией пользовательских эмбеддингов и проекцией внешних эмбеддингов\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, num_users, num_items, user_embedding_dim=1536, projection_dim=300, embedding_dim=50, num_heads=2, num_layers=2, dropout=0.2, max_len=50, user2embedding=None):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Эмбеддинги пользователей\n",
    "        self.user_embedding = nn.Embedding(num_users + 1, user_embedding_dim, padding_idx=0)\n",
    "        if user2embedding is not None:\n",
    "            self.init_user_embeddings(user2embedding)\n",
    "\n",
    "        # Проекция внешних эмбеддингов пользователей в пространство модели\n",
    "        self.external_projection = nn.Linear(user_embedding_dim, projection_dim)\n",
    "        self.user_projection = nn.Linear(projection_dim, embedding_dim)\n",
    "\n",
    "        # Эмбеддинги элементов и позиций\n",
    "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=0)  # +1 для паддинга\n",
    "        self.position_embedding = nn.Embedding(max_len, embedding_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
    "                                                   nhead=num_heads,\n",
    "                                                   dropout=dropout,\n",
    "                                                   activation='relu')\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(embedding_dim, num_items + 1)\n",
    "\n",
    "    def init_user_embeddings(self, user2embedding):\n",
    "        # Инициализация эмбеддингов пользователей из внешних данных\n",
    "        user_embeddings_tensor = torch.zeros(self.num_users + 1, external_embedding_dim)\n",
    "        for user_id, embedding in user2embedding.items():\n",
    "            if 0 < user_id <= self.num_users:\n",
    "                user_embeddings_tensor[user_id] = torch.tensor(embedding)\n",
    "        self.user_embedding.weight.data.copy_(user_embeddings_tensor)\n",
    "        # Если требуется, можно зафиксировать эмбеддинги пользователей\n",
    "        # self.user_embedding.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, user_ids, input_seq, seq_len):\n",
    "        \"\"\"\n",
    "        user_ids: (batch_size,)\n",
    "        input_seq: (batch_size, max_len)\n",
    "        seq_len: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Получение эмбеддингов пользователей\n",
    "        user_emb = self.user_embedding(user_ids)        # (batch_size, user_embedding_dim=1536)\n",
    "        user_emb = self.external_projection(user_emb)   # (batch_size, projection_dim=300)\n",
    "        user_emb = self.user_projection(user_emb)       # (batch_size, embedding_dim=50)\n",
    "        user_emb = user_emb.unsqueeze(1)                # (batch_size, 1, embedding_dim=50)\n",
    "\n",
    "        # Получение эмбеддингов элементов\n",
    "        item_emb = self.item_embedding(input_seq)       # (batch_size, max_len, embedding_dim=50)\n",
    "\n",
    "        # Получение эмбеддингов позиций\n",
    "        position_ids = torch.arange(0, input_seq.size(1), device=input_seq.device).unsqueeze(0).expand_as(input_seq)\n",
    "        pos_emb = self.position_embedding(position_ids)  # (batch_size, max_len, embedding_dim=50)\n",
    "\n",
    "        # Комбинирование эмбеддингов элементов, позиций и пользователей\n",
    "        combined_emb = item_emb + pos_emb                 # (batch_size, max_len, embedding_dim=50)\n",
    "        # Добавление пользовательского эмбеддинга к каждому элементу последовательности\n",
    "        user_emb_expanded = user_emb.expand(-1, input_seq.size(1), -1)  # (batch_size, max_len, embedding_dim=50)\n",
    "        combined_emb = combined_emb + user_emb_expanded             # (batch_size, max_len, embedding_dim=50)\n",
    "\n",
    "        # Нормализация и дропаут\n",
    "        combined_emb = self.layer_norm(combined_emb)\n",
    "        combined_emb = self.dropout(combined_emb)\n",
    "\n",
    "        # Transformer ожидает ввод формы (seq_len, batch_size, embedding_dim)\n",
    "        combined_emb = combined_emb.transpose(0, 1)  # (max_len, batch_size, embedding_dim=50)\n",
    "\n",
    "        # Создание маски для паддинга\n",
    "        mask = (input_seq == 0)  # (batch_size, max_len)\n",
    "\n",
    "        # Передача через Transformer\n",
    "        output = self.transformer(combined_emb, src_key_padding_mask=mask)  # (max_len, batch_size, embedding_dim=50)\n",
    "        output = output.transpose(0, 1)                                   # (batch_size, max_len, embedding_dim=50)\n",
    "\n",
    "        # Предсказание последнего элемента\n",
    "        output = output[:, -1, :]  # (batch_size, embedding_dim=50)\n",
    "        logits = self.fc(output)    # (batch_size, num_items + 1)\n",
    "\n",
    "        return logits, output      # Возвращаем логиты и внутренние эмбеддинги пользователей (embedding_dim=50)\n",
    "\n",
    "# Инициализация модели\n",
    "model = SASRec(num_users=num_users,\n",
    "              num_items=num_items,\n",
    "              user_embedding_dim=external_embedding_dim,  # 1536\n",
    "              projection_dim=projected_embedding_dim,    # 300\n",
    "              embedding_dim=embedding_dim,               # 50\n",
    "              num_heads=2,\n",
    "              num_layers=2,\n",
    "              dropout=0.2,\n",
    "              max_len=max_len,\n",
    "              user2embedding=user2embedding)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Определение функций потерь\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pos_scores, neg_scores):\n",
    "        \"\"\"\n",
    "        pos_scores: (batch_size, 1) — Предсказания для положительных элементов\n",
    "        neg_scores: (batch_size, 1) — Предсказания для негативных элементов\n",
    "        \"\"\"\n",
    "        # BPR Loss вычисляется как -log(sigmoid(pos - neg))\n",
    "        return -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, bpr_loss, alignment_loss, lambda_align=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bpr_loss = bpr_loss\n",
    "        self.alignment_loss = alignment_loss\n",
    "        self.lambda_align = lambda_align\n",
    "\n",
    "    def forward(self, pos_scores, neg_scores, internal_user_emb, projected_external_emb):\n",
    "        loss_bpr = self.bpr_loss(pos_scores, neg_scores)\n",
    "        loss_align = self.alignment_loss(internal_user_emb, projected_external_emb)\n",
    "        return loss_bpr + self.lambda_align * loss_align\n",
    "\n",
    "# Инициализация потерь и оптимизатора\n",
    "bpr_loss = BPRLoss()\n",
    "alignment_loss = nn.MSELoss()\n",
    "combined_criterion = CombinedLoss(bpr_loss, alignment_loss, lambda_align=lambda_align)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Функции метрик\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    recommended = recommended[:k]\n",
    "    hits = len(set(recommended) & set(relevant))\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    recommended = recommended[:k]\n",
    "    hits = len(set(recommended) & set(relevant))\n",
    "    return hits / len(relevant) if relevant else 0\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    recommended = recommended[:k]\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in relevant:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Функция обучения с использованием BPR Loss и alignment loss\n",
    "def train_epoch(model, loader, optimizer, combined_criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        user_ids, sequences, lengths = batch\n",
    "        user_ids = user_ids.to(device)      # (batch_size,)\n",
    "        sequences = sequences.to(device)    # (batch_size, max_len)\n",
    "        lengths = lengths.to(device)        # (batch_size,)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Предсказания для всех элементов\n",
    "        logits, internal_user_emb = model(user_ids, sequences, lengths)  # logits: (batch_size, num_items +1), internal_user_emb: (batch_size, embedding_dim=50)\n",
    "\n",
    "        # Положительные элементы — последний элемент в последовательности\n",
    "        pos_items = sequences[:, -1].unsqueeze(1)  # (batch_size, 1)\n",
    "        pos_scores = logits.gather(1, pos_items)   # (batch_size, 1)\n",
    "\n",
    "        # Выборка негативных элементов\n",
    "        neg_items = torch.randint(1, num_items + 1, pos_items.size(), device=device)\n",
    "        # Убедимся, что негативные элементы действительно негативные\n",
    "        for i in range(neg_items.size(0)):\n",
    "            while neg_items[i].item() in user_interactions[user_ids[i].item()]:\n",
    "                neg_items[i] = torch.randint(1, num_items + 1, (1,), device=device)\n",
    "\n",
    "        neg_scores = logits.gather(1, neg_items)   # (batch_size, 1)\n",
    "\n",
    "        # Получение внешних эмбеддингов пользователей и проекция их\n",
    "        external_emb = torch.tensor([user2embedding[user_id.item()] for user_id in user_ids], dtype=torch.float32).to(device)  # (batch_size, external_embedding_dim=1536)\n",
    "        projected_external_emb = model.user_projection(model.external_projection(external_emb))  # (batch_size, embedding_dim=50)\n",
    "\n",
    "        # Вычисление комбинированного лосса\n",
    "        loss = combined_criterion(pos_scores, neg_scores, internal_user_emb, projected_external_emb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Функция валидации с использованием BPR Loss и alignment loss\n",
    "def validate(model, loader, combined_criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            user_ids, sequences, lengths = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            sequences = sequences.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Предсказания для всех элементов\n",
    "            logits, internal_user_emb = model(user_ids, sequences, lengths)  # (batch_size, num_items +1), (batch_size, embedding_dim=50)\n",
    "\n",
    "            # Положительные элементы — последний элемент в последовательности\n",
    "            pos_items = sequences[:, -1].unsqueeze(1)  # (batch_size, 1)\n",
    "            pos_scores = logits.gather(1, pos_items)   # (batch_size, 1)\n",
    "\n",
    "            # Выборка негативных элементов\n",
    "            neg_items = torch.randint(1, num_items + 1, pos_items.size(), device=device)\n",
    "            # Убедимся, что негативные элементы действительно негативные\n",
    "            for i in range(neg_items.size(0)):\n",
    "                while neg_items[i].item() in user_interactions[user_ids[i].item()]:\n",
    "                    neg_items[i] = torch.randint(1, num_items + 1, (1,), device=device)\n",
    "\n",
    "            neg_scores = logits.gather(1, neg_items)   # (batch_size, 1)\n",
    "\n",
    "            # Получение внешних эмбеддингов пользователей и проекция их\n",
    "            external_emb = torch.tensor([user2embedding[user_id.item()] for user_id in user_ids], dtype=torch.float32).to(device)  # (batch_size, external_embedding_dim=1536)\n",
    "            projected_external_emb = model.user_projection(model.external_projection(external_emb))  # (batch_size, embedding_dim=50)\n",
    "\n",
    "            # Вычисление комбинированного лосса\n",
    "            loss = combined_criterion(pos_scores, neg_scores, internal_user_emb, projected_external_emb)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Функция оценки модели с негативными сэмплами\n",
    "def evaluate_model_with_negatives(model, loader, device, k=10):\n",
    "    model.eval()\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Каждый батч содержит одного пользователя\n",
    "            user_id, seq, seq_len, items, pos_item = batch\n",
    "            user_id = user_id.to(device)      # (1,)\n",
    "            seq = seq.to(device)              # (1, max_len)\n",
    "            seq_len = seq_len.to(device)      # (1,)\n",
    "            items = items.to(device)          # (1, num_negatives +1)\n",
    "            pos_item = pos_item.to(device)    # (1,)\n",
    "\n",
    "            # Получение эмбеддингов и прогнозов модели для всех элементов\n",
    "            logits, _ = model(user_id, seq, seq_len)  # (1, num_items +1)\n",
    "\n",
    "            # Извлечение оценок только для выбранных элементов (негативные + положительный)\n",
    "            # Предполагается, что items содержат индексы элементов\n",
    "            item_scores = logits.gather(1, items)  # (1, num_negatives +1)\n",
    "\n",
    "            # Получение top-K элементов среди негативных и положительного\n",
    "            _, topk_indices = torch.topk(item_scores, k, dim=1)  # (1, k)\n",
    "            topk_items = items[0][topk_indices[0]].cpu().numpy()\n",
    "\n",
    "            # Положительный элемент находится в конце списка items\n",
    "            recommended = topk_items\n",
    "            relevant = [pos_item.item()]\n",
    "\n",
    "            precision_scores.append(precision_at_k(recommended, relevant, k))\n",
    "            recall_scores.append(recall_at_k(recommended, relevant, k))\n",
    "            ndcg_scores.append(ndcg_at_k(recommended, relevant, k))\n",
    "\n",
    "    # Вычисление средних значений метрик\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_ndcg = np.mean(ndcg_scores)\n",
    "\n",
    "    return mean_precision, mean_recall, mean_ndcg\n",
    "\n",
    "# Цикл обучения с валидацией\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, combined_criterion, device)\n",
    "    valid_loss = validate(model, valid_loader, combined_criterion, device)\n",
    "    print(f'Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "# Параметр K для top-K рекомендаций\n",
    "k = 10\n",
    "\n",
    "# Оценка модели на тестовом наборе с негативными сэмплами\n",
    "precision, recall, ndcg = evaluate_model_with_negatives(model, test_eval_loader, device, k=k)\n",
    "print(f'Precision@{k}: {precision:.4f}')\n",
    "print(f'Recall@{k}: {recall:.4f}')\n",
    "print(f'NDCG@{k}: {ndcg:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 negative sampling\n",
    "\n",
    "| Метрика         | Baseline | Transfer Learning |\n",
    "|-----------------|----------|-------------------|\n",
    "| Precision@10    | 0.0044   | 0.0075           |\n",
    "| Recall@10       | 0.0444   | 0.0747           |\n",
    "| NDCG@10         | 0.0211   | 0.0375           |\n",
    "\n",
    "# 300 negative sampling\n",
    "\n",
    "| Метрика         | Baseline | Transfer Learning |\n",
    "|-----------------|----------|-------------------|\n",
    "| Precision@10    | 0.0120   | 0.0184           |\n",
    "| Recall@10       | 0.1199   | 0.1841           |\n",
    "| NDCG@10         | 0.0571   | 0.0917           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
